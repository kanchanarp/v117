---
title: Bandit Algorithms Based on Thompson Sampling for Bounded Reward Distributions
abstract: We focus on a classic reinforcement learning problem, called a multi-armed
  bandit, and more specifically in the stochastic setting with reward distributions
  bounded in $[0,1]$. For this model, an optimal problem-dependent asymptotic regret
  lower bound has been derived. However, the existing algorithms achieving this regret
  lower bound all require to solve an optimization problem at each step, inducing
  a large complexity. In this paper, we propose two new algorithms, which we prove
  to achieve the problem-dependent asymptotic regret lower bound. The first one, which
  we call Multinomial TS, is an adaptation of Thompson Sampling for Bernoulli rewards
  to multinomial reward distributions whose support is included in $\{0, \frac{1}{M},
  â€¦, 1\}$. This algorithm achieves the regret lower bound in the case of multinomial
  distributions with the aforementioned support, and it can be easily generalized
  to bounded reward distributions in $[0, 1]$ by randomly rounding the observed rewards.
  The second algorithm we introduce, which we call Non-parametric TS, is a randomized
  algorithm but not based on the posterior sampling in the strict sense. At each step,
  it computes an average of the observed rewards with random weight. Not only is it
  asymptotically optimal, but also it performs very well even for small horizons.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: riou20a
month: 0
tex_title: Bandit Algorithms Based on Thompson Sampling for Bounded Reward Distributions
firstpage: 777
lastpage: 826
page: 777-826
order: 777
cycles: false
bibtex_author: Riou, Charles and Honda, Junya
author:
- given: Charles
  family: Riou
- given: Junya
  family: Honda
date: 2020-01-28
address: 
publisher: PMLR
container-title: Proceedings of the 31st International Conference  on Algorithmic
  Learning Theory
volume: '117'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 1
  - 28
pdf: http://proceedings.mlr.press/v117/riou20a/riou20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
