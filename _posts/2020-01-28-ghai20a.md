---
title: Exponentiated Gradient Meets Gradient Descent
abstract: " The (stochastic) gradient descent and the multiplicative update method
  are  probably the most popular algorithms in machine learning. We introduce and
  study a new regularization which provides a unification of the additive and multiplicative
  updates. This regularization is derived from an hyperbolic analogue of the entropy
  function, which we call hypentropy. It is motivated by a natural extension of the
  multiplicative update to negative numbers. The hypentropy has a natural spectral
  counterpart which we use to derive a family of matrix-based updates that bridge
  gradient methods and the multiplicative method for matrices. While the latter is
  only applicable to positive semi-definite matrices, the spectral hypentropy method
  can naturally be used with general rectangular matrices. We analyze the new family
  of updates by deriving tight regret bounds. We study empirically the applicability
  of the new update for settings such as multiclass learning, in which the parameters
  constitute a general rectangular matrix."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ghai20a
month: 0
tex_title: Exponentiated Gradient Meets Gradient Descent
firstpage: 386
lastpage: 407
page: 386-407
order: 386
cycles: false
bibtex_author: Ghai, Udaya and Hazan, Elad and Singer, Yoram
author:
- given: Udaya
  family: Ghai
- given: Elad
  family: Hazan
- given: Yoram
  family: Singer
date: 2020-01-28
address: 
publisher: PMLR
container-title: Proceedings of the 31st International Conference  on Algorithmic
  Learning Theory
volume: '117'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 1
  - 28
pdf: http://proceedings.mlr.press/v117/ghai20a/ghai20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
