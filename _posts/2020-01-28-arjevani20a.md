---
title: A Tight Convergence Analysis for Stochastic Gradient  Descent with Delayed
  Updates
abstract: 'We establish matching upper and lower complexity bounds for  gradient descent
  and stochastic gradient descent on quadratic functions,  when the gradients are
  delayed and reflect iterates from $\tau$ rounds ago.  First, we show that without
  stochastic noise, delays strongly affect  the attainable optimization error: In
  fact, the error can be as bad as  non-delayed gradient descent ran on only $1/\tau$
  of the gradients. In  sharp contrast, we quantify how stochastic noise makes the
  effect of  delays negligible, improving on previous work which only showed this  phenomenon
  asymptotically or for much smaller delays. Also, in the  context of distributed
  optimization, the results indicate that the  performance of gradient descent with
  delays is competitive with  synchronous approaches such as mini-batching. Our results
  are based on  a novel technique for analyzing convergence of optimization algorithms  using
  generating functions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: arjevani20a
month: 0
tex_title: A Tight Convergence Analysis for Stochastic Gradient  Descent with Delayed
  Updates
firstpage: 111
lastpage: 132
page: 111-132
order: 111
cycles: false
bibtex_author: Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan
author:
- given: Yossi
  family: Arjevani
- given: Ohad
  family: Shamir
- given: Nathan
  family: Srebro
date: 2020-01-28
address: 
publisher: PMLR
container-title: Proceedings of the 31st International Conference  on Algorithmic
  Learning Theory
volume: '117'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 1
  - 28
pdf: http://proceedings.mlr.press/v117/arjevani20a/arjevani20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
